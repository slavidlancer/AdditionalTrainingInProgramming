
function slide=upr3
% This is a slideshow file for use with playshow.m and makeshow.m
% To see it run, type 'playshow upr3', 

% Copyright (c) 1984-98 by The MathWorks, Inc.
if nargout<1,
  playshow upr3
else
  %========== Slide 1 ==========

  slide(1).code={
   'DATA = imread(''upr312.bmp'');',
   'imshow(DATA);',
   'imwrite(DATA, ''myfile.bmp'');',
   '',
   '' };
slide(1).text={
   '',
   '',
   '                                 Лабораторно уптажнение  №3',
   '',
   '',
   '',
   '                МРЕЖИ С ОБРАТНО РАЗПРОСТРАНЕНИЕ НА ГРЕШКАТА'};
%========== Slide 2 ==========

  slide(2).code={
   'DATA = imread(''upr312.bmp'');',
   'imshow(DATA);',
   'imwrite(DATA, ''myfile.bmp'');',
   '',
   '' };
  slide(2).text={
   'Теоретична част:',
   '',
   '   Невронни мрежи с обратно разпространение на грешката, представляват невронни мрежи с многослойна структура, при чието обучение се използва алгоритъм за обратно разпространение на грешката, който променя тегловите коефициенти на различните слоеве на мрежата. Многослойните мрежи имат един входен слой Х, където се подават сигналите, един или повече скрити /вътрешни/ слоеве Z и един изходен слой Y. Тегловните коефициенти се настройват /обучават/ като се използва обратното раппространение на грешката - този метод се среща в специализираните  '};

  %========== Slide 3 ==========

  slide(3).code={
   'DATA = imread(''upr35.bmp'');',
   'imshow(DATA);',
   'imwrite(DATA, ''myfile.bmp'');',
   '',
   '' };

  slide(3).text={
   'издания като обобщен метод на най-малките квадрати.Тези невронни мрежи са базирани на нелинейни неврони с гладка активационна функция, каквато например са сигмоидалната и хиперболичната сигмоидална функции, представени на фиг.1 и фиг.2.',
   '   Алгоритъма за обучение на мрежата се състои от два етапа - в първия етап пресмятаме изходите на невроните от последния слой и се пресмята грешката. Във втория етап се пресмята измененията на тегловите коефициенти, като настройката започва от последния изходен слой обратно към входния. Теглата се настройват слой след слой.'
   '    Алгоритъм на мрежите с обратно разпределение на грешката:',
};

  %========== Slide 4 ==========

  slide(4).code={
   'DATA = imread(''upr36.bmp'');',
   'imshow(DATA);',
   'imwrite(DATA, ''myfile.bmp'');',
   '',
   '' };

slide(4).text={
      '      1)Първоначална инициализация на матриците w и v, с тегловните коефициенти и отклоненията.',
   '      2)Ако условията за прекратяване на алгоритъма не са изпълнени се правят стъпки 3:10',
   '      3)За всеки елемент на извадката за обучение x:t се правят стъпки 4:8',
   '      4)Прилага се даден вектор на входния слой',
   '      5)Изчислява се входа и изхода на всеки елемент от скритите слоеве Z_j, по формула (2)',
	'      6) Пресмята се входа и изхода на всеки елемент от изходния слой Y_k, по формула (3);',
   
};

  %========== Slide 5 ==========

  slide(5).code={
   'DATA = imread(''upr37.bmp'');',
   'imshow(DATA);',
   'imwrite(DATA, ''myfile.bmp'');',
   '',
   '' };

slide(5).text={
   '  7)За всеки елемент от изходния слой Y_k се пресмята грешката, по формула (4) и се променят тегловните коефициенти по формула (5);',
   '  8)За всички елементи на скритите слоеве Z_j се прави обратно разпространение на грешката по формули (6) и (7) и се променят тегловните коефициенти по формула (8);',
   '  9)Обновяват се тегловите коефициенти (включително отместванията), по формула (9);',
   '  10)Край на алгоритъма',
   '',
   '',
   ''};


  %========== Slide 6 ==========
  slide(6).code={
  'DATA = imread(''upr312.bmp'');',
   'imshow(DATA);',
   'imwrite(DATA, ''myfile.bmp'');',
};
  slide(6).text={
     'Цел на лабораторното упражнение:',
     '',
     '   Да се генерират 50 входно-изходни двойки данни за обучение на двуслойна невронна мрежа с обратно разпространение на грешката с 4 входа и 3 изхода. Да се използват функциите y1=x12+x22+x1x2x3+2, y2=x1x4-x2x3+2, y3=(x3/x4)+x32x1+2, като теглата за х1..хn са съответно W=[2 5 3 4].',
     ' Да се избере подходящ брой неврони в скрития слой и да се тества за колко "epoch" невронната мрежа може да се обучи при достигане на средноквадратична грешка 0,01.Ако това условие не бъде изпълнено да се '};

  %========== Slide 7 ==========

  slide(7).code={
   '' };
slide(7).text={
   'обучи мрежата с друг алгоритъм, с други начални стойности на теглата или с друг брой неврони в скрития слой.',
   '      Да се тества мрежата с 10 входно-изходни двойки, получени със същите функции, посредством които е генерирана обучаващата извадка.'};

  %========== Slide 8 ==========

  slide(8).code={
   'nntwarn off',
   'w=4;',
   'b=2;',
   'P=rand(4,50);',
   'T1=(P(1,:).^2)./(w*P(1,:))+b',
   'T2=(P(2,:).^2)./(w*P(2,:))+b',
   'T3=(P(3,:).^2)./(w*P(3,:))+b',
   'T4=(P(1,:).^2)./(w*P(1,:))+b',
   'T5=(P(1,:).^2)./(w*P(1,:))+b',
   'T6=(P(2,:).^2)./(w*P(4,:))+b',
   'T=[T1;T2;T3;T4;T5;T6]',
   '[R,Q] = size(P); S1 =3 ; [S2,Q] = size(T);',
   '[W10,B10] = rands(S1,R);',
   '[W20,B20] = rands(S2,S1);',
   'disp_freq = 10;',
   'max_epoch = 200;',
   'err_goal = 0.02;',
   'lr = 0.01;',
   'TP = [disp_freq max_epoch err_goal lr];',
   '[W1,B1,W2,B2,epoch,errors] =trainbp(W10,B10,''tansig'',W20,B20,''purelin'',P,T,TP)',
   'W1',
   'B1',
   'W2',
   'B2',
   'A1 = tansig(W1*P,B1);',
   'A2 = purelin(W2*A1,B2);',
   'E = T-A2;',
   'SSE1 = sumsqr(E)',
   '' };
  slide(8).text={
   'Начин на провеждане на лабораторното упражнение:',
   '',
   '  Обучете невронната мрежа с обратно разпространение на грешката до',
   'достигане на зададената желана точност при следните видве структури:',
   'nntwarn off',
   '%w=4',
   '%b=2',
   '%P=rand(4,50)',
   '%T1=(P(1,:).^2)./(w*P(1,:))+b',
   '%T2=(P(2,:).^2)./(w*P(2,:))+b',
   ''};

  %========== Slide 9 ==========

  slide(9).code={
   '' };
  slide(9).text={
   '%T3=(P(3,:).^2)./(w*P(3,:))+b',
   '%T4=(P(1,:).^2)./(w*P(1,:))+b',
   '%T5=(P(1,:).^2)./(w*P(1,:))+b',
   '%T6=(P(2,:).^2)./(w*P(4,:))+b',
   '%T=[T1;T2;T3;T4;T5;T6]',
   ' Задаваме арихитектурата на невронната мрежа, чрез R се задава големината на входния вектора, a чрез S1 и S2 се задава броя неврони във втория слой',
   '%[R,Q] = size(P); S1 =3 ; [S2,Q] = size(T);',
   '%[W10,B10] = rands(S1,R);',
   
   ''};

  %========== Slide 10 ==========

  slide(10).code={
   '' };
slide(10).text={
   '%[W20,B20] = rands(S2,S1);',
   ' Задаваме параметрите като брой епохи, желана точност, скорост на обучение',
   '%disp_freq = 5;',
   '%max_epoch = 5000;',
   '%err_goal = 0.02;',
   '%lr = 0.01;',
   '%TP = [disp_freq max_epoch err_goal lr];',
   '%[W1,B1,W2,B2,epoch,errors',
      '',
   ''};

  %========== Slide 11 ==========

  slide(11).code={
   '' };
slide(11).text={
   'Обучение на невронната мрежа:',
   '%=trainbp(W10,B10,''tansig'',W20,B20,''purelin'',P,T,TP)',
   '%A1 = tansig(W1*P,B1);',
   '%A2 = purelin(W2*A1,B2);',
   '%E = T-A2;',
   '%SSE1 = sumsqr(E)'};
end